---
layout: post
title: 딥러닝 기반의 multivariate time series anomaly detection
subtitle: omnianomaly, usad, gdn, thoc, madgan
cover-img: /assets/img/axp/black.jpg
thumbnail-img: /assets/img/axp/thumb.png
share-img: /assets/img/axp/black.jpg
tags: [ml, papers, time_series, anomaly_detection]
mathjax: true
---

이번 글에서는 multivariate time series 데이터에서 anomaly detection을 진행하는 방법 중 딥러닝 기반의 방법들에 대해 알아보겠습니다.  
뛰어난 논문들이 많지만 아래 5개의 알고리즘에 대해 정리해 봅니다.  

- [MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks, 2019](https://arxiv.org/abs/1901.04997)  
- [OmniAnomaly: Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network, 2019](https://www.semanticscholar.org/paper/Robust-Anomaly-Detection-for-Multivariate-Time-Su-Zhao/440d248d148f7e36dad232e48f1c5c1cbc556d86)  
- [USAD: UnSupervised Anomaly Detection on multivariate time series, 2020](https://www.kdd.org/kdd2020/accepted-papers/view/usad-unsupervised-anomaly-detection-on-multivariate-time-series)  
- [THOC: Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network, 2020](https://proceedings.neurips.cc/paper/2020/file/97e401a02082021fd24957f852e0e475-Paper.pdf)  
- [GDN: Graph Neural Network-Based Anomaly Detection in Multivariate Time Series, 2021](https://arxiv.org/pdf/2106.06947.pdf)  

시계열 데이터에서 이상 탐지를 위해 다양한 방법이 연구되었지만 과거의 많은 연구들은 univariate time series 데이터에 한정하는 경우가 많았습니다. 왜냐하면 dimension이 증가할 수록 classical한 방법들로는 그 사이의 interaction을 효과적으로 포착하기도 어려웠고, 개별적으로 modeling을 진행하기에는 리소스도 많이 들고 데이터의 손실 역시 컸기 때문입니다.  

이러한 문제점을 해결하기 위한 방안으로 수년 전부터 딥러닝 기반의 알고리즘들이 제안되어 왔습니다. 

## GDN: Graph Neural Network-Based Anomaly Detection in Multivariate Time Series  
고차원의 데이터셋, 즉 다변량 데이터에서 변수(sensor) 사이의 interaction을 잘 포착하는 것이 multivariate time series forecasting의 핵심 요소입니다. classical auto-regressive model 모델의 경우 복잡한 비선형 특성을 모델링하기에 적합하지 않습니다. CNN, LSTM, GAN 등의 구조에 기반한 딥러닝 방법론 역시 등장했지만 본 논문에서는 이 역시 변수 간 관계를 명확히 학습하기에는 부족하다고 이야기합니다. 그리고 대안으로 GNN을 접목한 방법론을 제시합니다. GNN에 대한 기반 지식이 부족하다면 [이 곳](https://greeksharifa.github.io/machine_learning/2020/12/31/Graph-Sage/)에 있는 몇 가지 글을 읽어보셔도 좋을 것 같습니다.  

![structure](/assets/img/mtsad/gdn1.jpg){: .mx-auto.d-block :}

데이터 형상은 위와 같습니다. 기본적으로 본 논문에서 제시하는 방법은 `prediction error`를 최소화하는 방법입니다. $w$ 크기의 window size를 설정하고 이를 sliding 하면서 학습/테스트 데이터를 생성합니다. 결과적으로 $N$ 개의 sensor에 기반하여 $t$ 시점의 모델은 binary classification을 수행하게 됩니다. (이상치 여부를 0 또는 1로 나타냄)

참고로 논문에서 사용한 **SWaT, WADI** 데이터셋의 경우 이상치 비율이 각각 11.97%, 5.99% 인데 실제 데이터에서는 이보다 이상치 비율이 현저히 적은 경우가 많을 것이라고 예상됩니다. 따라서 label로 설정할 수 있는 이상치 데이터가 충분한 경우에만 `GDN`을 그대로 이용할 수 있을 것입니다.  

`GDN`은 다음과 같이 구성되어 있습니다.  
- sensor embedding  
- graph structure learning  
- graph attention-based forecasting  
- graph deviation scoring  

![structure](/assets/img/mtsad/gdn2.PNG){: .mx-auto.d-block :}  

여러 단계로 나와있습니다만 input 데이터에 있는 $N$ 개의 sensor의 시계열 데이터를 각각 임베딩 벡터로 나타내고 이를 directed graph network의 node feature로 사용합니다. 그리고 $N$ 개의 모든 sensor에 대해 graph attention 구조로 message passing & aggregation을 수행합니다. 그렇게 되면 총 $N$ 개의 벡터가 결과물로 산출될 것입니다.  

이들은 다시 graph attention을 적용하기 이전의 벡터와 element-wise 곱을 수행한 뒤 fully-connected layer를 통과하여 $N$ 의 길이를 갖는 vector output으로 출력됩니다. 그리고 이 예측값으로 binary classification을 수행하는 것입니다.  

$$ \hat{s}^t = f_{\theta} ([\mathbb{v}_1 * \mathbb{z}_1^t, ..., \mathbb{v}_N * \mathbb{z}_N^t]) $$  

구조는 매우 직관적입니다. time series data를 하나의 embedding vector로 표현하고, 이 vector 사이의 interaction을 포착하기 위해 graph attention network를 이용합니다. 그리고 이들을 모두 모아 하나의 layer를 통과시켜 최종 예측 값을 얻습니다.  

이제 실제 상황에서 어떻게 이상치를 판별할지 알아봅시다. 논문에서는 이 과정을 graph deviation scoring이라고 명명하고 있습니다.  

sensor $i$ 의 time $t$ 에서의 에러 값은 아래와 같이 정의할 수 있습니다.  

$$ Err_i(t) = \vert s_i^t - \hat{s}_i^t \vert $$  

각 sensor의 특징은 제각각이므로 그들의 deviation은 서로 다른 scale을 갖고 있을 수 있습니다. 즉 정규화가 되어 있지 않은 상황이므로 다음과 같이 **anomalousness score**에 `robust normalization`을 적용해 줍니다.  

$$ a_i(t) = \frac {Err_i^t - \tilde{\mu}_i} {\tilde{\sigma}_i} $$  

이 때 $\tilde{\sigma}_i$ 는 median을, $\tilde{\mu}_i$ 는 IQR의 제곱을 의미합니다. 이렇게 $N$ 개의 sensor에 대해 모두 **anomalousness score**를 얻은 후 max 함수를 통해 대표값을 산정합니다.  

$$ A(t) = \text{max}_t a_i (t) $$  

결과값의 갑작스러운 변화를 완화하기 위해서 SMA: simple moving average 기법을 적용하여 최종적으로 smooted score $A_s (t)$ 를 구합니다. 그리고 이 score가 특정 임계값을 넘으면 이상치라고 판별하게 됩니다. 이 때 threshold를 정하는 방법은 굉장히 다양한데, 본 논문에서는 이를 정하는 데에 있어 hyper-parameter를 남겨두고 싶지 않았기 때문에 validation data를 이용해서 threshold를 정했다고 서술하고 있습니다.  

실험 결과를 살펴보면, PCA, KNN과 같은 classic한 알고리즘부터 AE, DAGMM, LSTM-VAE, MAD-GAN 등 수 년 사이에 등장한 딥러닝 알고리즘들과도 비교하고 있습니다. 보통의 논문과 마찬가지로 precision, recall, f1-score을 이용하여 평가를 진행하였습니다.  

![structure](/assets/img/mtsad/gdn3.PNG){: .mx-auto.d-block :}  

