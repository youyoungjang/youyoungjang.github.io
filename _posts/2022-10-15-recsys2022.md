---
layout: post
title: Recsys 2022 paper 리뷰
subtitle: 주요 논문을 살펴보고 응용 방안에 대해 생각해봅니다.
cover-img: /assets/img/recsys/cover.jpg
thumbnail-img: /assets/img/recsys/og.jpg
share-img: /assets/img/recsys/cover.jpg
tags: [recsys, ml, papers]
mathjax: true
---

본 글에서는 [Recsys2022](https://recsys.acm.org/recsys22/accepted-contributions/)에서 확인할 수 있었던 흥미로운 논문들 중 일부를 리뷰하고 정리해보도록 하겠습니다.  

## Papers  
### Two-layer Bandit Optimization for Recommendations (Apple)  
- [논문 원본](https://dl.acm.org/doi/abs/10.1145/3523227.3547396)  

app marketplace에서 수 많은 앱 목록을 개인화된 알고리즘을 통해 노출시킨다고 하면 여러 어려움에 직면할 수 있습니다. item(앱) 종류가 매우 다양하고 시시각각으로 변한다는 것 외에도 cannibalization이 발생할 수 있습니다. 예를 들어 특정 앱을 우선적으로 추천한다고 할 때 어쩌면 어떤 user는 그 item을 다운로드 받은 후 더 이상 필요한 앱을 찾아보지 않고 marketplace를 떠나버릴지도 모릅니다.  

**apple**에서는 이러한 문제를 해결하기 위해 굉장히 이해하기 쉬우면서도 효과적인 추천 시스템을 설계했습니다. 그들의 목표는 전체적인 engagement를 저해하지 않으면서도 **추천한(Suggested)** item의 engagement를 증가시키는 것이었습니다.  

논문에서는 `Two-layer Bandit` 구조가 제안되었습니다. 아래와 크게 `Bandit Recall Layer`와 `Bandit Ranking Layer`로 구성됩니다.  

![structure](/assets/img/recsys/apple.PNG){: .mx-auto.d-block :}

전자는 candidate들을 선정하고, 후자는 최종 랭킹을 결정하게 됩니다. update를 위해 **cohort-level effective rewards** 데이터가 수집됩니다. cohort-level로 reward를 계산한다는 부분이 색다르게 느껴집니다. user-level이 아니라 cohort-level로 reward를 계산하게 되면 어쩌면 완벽한 초개인화를 달성하는 것은 힘들 수 있다 하더라도 실제 시스템을 운영할 때의 효율은 훨씬 향상될 수도 있을 것입니다. 유사한 취향을 가진 고객들을 `K-Means` 알고리즘을 통해 grouping하고 이를 바탕으로 cohort를 구성합니다.  

논문에서는 **effective rewards**를 아래와 같이 정의하였습니다.  

$$ F_{\text CC}(i, C_j) = {\text max} (S^i_{C_j} - D^i_{C_j}, 1) $$  
 
여기서 $i$ 는 item index를 의미하고, $C_j$ 는 $j$ 번째 cohort를 의미합니다. $S$ 는 검색을 한 번이라고 하고 item을 다운로드 받은 적이 있는 user session의 수를 의미하고 $D$ 는 검색 없이 다운로드 한 user session의 수를 의미합니다. 즉 이러한 reward 설계는 cannibalization을 방지하겠다는 의도를 담고 있습니다.  

만약에 cannibalization을 고려할 필요가 없다고 한다면 $S$ 만 고려해도 될 것입니다. 혹은 다운로드 수 뿐만 아니라 좀 더 다양한 요소를 반영하고 싶다면 reward 식을 더 복잡하게 개량할 수도 있을 것입니다.  

**Bandit Recall Layer**의 핵심은 각 cohort에게 너무 어울리지 않을 법한 item을 빼고 추천 후보 리스트를 만드는 것입니다. 이 때의 기준은 `conversion rate`이 됩니다.  

각 cohort-item pair에 대하여 로그 데이터를 활용하여 모든 empirical conversion rate을 구해줍니다. 그리고 conversion rate이 이항 분포를 통해 형성되었다고 가정하고 CLT를 활용하여 정규분포 형태로 바꿔줍니다. 그리고 여기서 각 cohort-item pair에 대해 `95% LCB: Lower Confidence Bound`를 계산하고 특정 임계값을 넘는 pair만 남겨둡니다. 그리고 각 cohort에 대해 후보 item들의 선택받을 확률을 softmax 함수를 통해 구성해주면 됩니다. 논문에서는 수백 개 정도의 item을 남겨두었다고 기술하고 있습니다.  

보통 `UCB: Upper Confidence Bound`를 적용하는 사례를 많이 보게 되는데, 이 논문에서는 `LCB: Lower Confidence Bound`를 적용하였습니다. 왜냐하면 이 단계는 최종 단계가 아닌 item 후보 목록을 선정하는 단계이기 때문에 가장 relevant한 item을 찾는 것보다 irrelvant한 item을 걸러내는 것이 더 중요한 작업이기 때문입니다. (상대적으로 더 보수적인 접근 방법입니다.)  

LCB Sampling 과정이 끝나면 `Thompson Sampling` 전략이 적용된 **Bandit Ranking Layer**가 이어집니다. 샘플링은 다음과 같은 분포에 의거하여 이루어집니다.  

$$ {\text Beta}(\alpha = F_{CC}(i, C_j), \beta = I^i_{C_j}) $$  

위 과정을 통해 relevant한 item을 선별하고 추가적으로 uniform sampling을 통해 cold item이 소외받지 않도록 해줍니다. 이를 통해 모델의 confirmation bias를 줄일 수 있습니다. 계속 같은 item을 보여줄 수는 없으니까요.  

실험의 결과는 물론 단순 uniform sampling 보다 위와 같은 `Two-layer Bandit` 구조가 우수하다는 것을 증명합니다. 그런데 흥미로운 부분은, LCB와 달리 Thompson Sampling만을 적용했을 때 Cannibalization 현상이 발생했다는 점입니다.  

![table](/assets/img/recsys/apple2.PNG){: .mx-auto.d-block :}

만약 Thompson Sampling만을 이용했다면 비록 relevance는 올라갔을 수 있지만 실제로 서비스에서 추구하는 결과와는 좀 멀어졌을 수도 있겠다는 생각이 드는 대목이었습니다. LCB sampling와 uniform sampling을 함께 적용함으로써 균형적인 결과를 얻을 수 있었던 것으로 보입니다.  

## Extending Open Bandit Pipeline to Simulate Industry Challenges  
- [논문 원본](https://arxiv.org/abs/2209.04147)  

이 논문은 새로운 알고리즘을 소개하는 논문이 아닙니다. 실제 현실에서 Bandit Optimization 문제를 풀기 위해서 맞닥드리는 여러 상황과 의문점을 제시하고 이를 실험을 통해 더욱 쉽게 접근할 수 있도록 오픈 소스 라이브러리를 확장했다는 이야기를 담고 있습니다.  

[OBP: Open Bandint Pipeline](https://github.com/st-tech/zr-obp)은 off-policy learning & evaluation 을 위한 라이브러리입니다. [논문](https://arxiv.org/abs/2008.07146) 또한 존재합니다.  

논문에서는 크게 5가지의 주요 문제를 언급합니다.  
- on-policy, off-policy 둘 중 어떤 알고리즘을 선택할 것인가?  
- delayed reward는 어떻게 다룰 것인가?  
- 다양한 유형의 drift는 어떻게 다룰 것인가? (빠르게 environment가 변화하는 현상)  
- relastic reward는 어떻게 정의하는가?  
- 수많은 비즈니스 규칙과 끈임없이 변화하는 arm 속에서 어떻게 bandit 문제를 최적화할 것인가?  

정말 머리 아픈 문제들이 틀림 없습니다. 논문에서는 몇 가지 가이드를 제시하고는 있지만, 골자는 결국 실험을 통해 알아보는 것이 최선이라는 것입니다. **OBP**를 활용하면 좋겠네요.  

### On-policy vs Off-policy  

일반적으로 bandit 알고리즘들은 On-policy로 최적화합니다. 즉 각 step(round) 마다 policy로 부터 action을 추출해서 explore/exploit 하게 됩니다. Off-policy는 반대로 policy를 기존에 존재하는 데이터로 부터 학습하고 그 후 production에 세우게 됩니다. 보통은 `IPW: Inverse-propensity Weighting` 을 이용하여 로그 데이터의 편향을 줄이는 방식으로 구현합니다.  

On-policy는 실제 상황에서 적용하기에는 초기 비용이 크다는 단점이 있습니다. 충분한 exploration이 이루어지는 동안 실제 비즈니스 비용은 계속 지출될 것이니까요. 그리고 user experience를 해칠 수도 있습니다.  

논문 부록에 첨부된 실험 결과를 한 번 보겠습니다.  

![table](/assets/img/recsys/obp1.PNG){: .mx-auto.d-block :}

IPW를 활용하여 off-policy로 학습한 결과가 더욱 좋습니다. 이는 꼭 처음부터 On-policy를 고집할 이유는 없다는 것을 증명한 사례이기도 합니다. 직접 실험해보면 좋겠네요.  

![table](/assets/img/recsys/obp2.PNG){: .mx-auto.d-block :}

logging policy에 따른 성능 차이도 보여줍니다. 높은 exploration의 egreedy_1.0이 가장 우수한 결과를 이끌어 냅니다.  

### Concept Drift & Non-stationarity  

Drift와 Non-stationarity 문제는 현실적으로 거의 무조건 마주칠 수 밖에 없는 문제입니다. 아마 drift 후에 얼마나 빠르게 on-policy 방법론은 회복할 수 있을지, 어떠한 종류의 drift가 가장 challenging할지, off-policy 방법론을 어떻게 non-stationary 환경에 적용할지 등의 문제를 마주하게 될 것입니다.  

![table](/assets/img/recsys/obp3.PNG){: .mx-auto.d-block :}

논문에서는 갑작스런 drift로 인한 성능저하와 seasonal drift로 인한 성능저하 및 회복 기간에 대한 인사이트를 제공합니다.  

### Delayed Reward  

리워드가 지연되어 지급되는 경우는 굉장히 빈번합니다. 구매 확정이 며칠 뒤에 발생하는 것이 대표적인 예일 것입니다. 이러한 상황은 어떠한 영향을 끼칠지 시뮬레이션 해보는 것은 중요한 문제입니다.  

![table](/assets/img/recsys/obp4.PNG){: .mx-auto.d-block :}

논문에서의 실험을 보면, 초반에는 delayed reward로 인한 격차가 눈에 띄게 보이지만, 일정 시점 이후에는 거의 같아진다는 것을 보여줍니다. 실제 적용 상황을 생각해본다면, off-policy 학습을 통해 어느 정도의 step 수가 필요한지 가늠해 볼 수도 있을 것입니다.  

### Reward Design and Multi-Reward  

business objective와 reward가 정확히 일치하는 것은 상당히 이상적인 시나리오에서나 가능합니다. 보통은 sparsity, delay나 여러 복잡한 상황들로 인해 괴리가 있는 것이 사실입니다. 논문에서는 sub-optimal 한 결과로 이어지지 않으면서도 어떻게 잘 최적화할 수 있을지에 대한 질문을 던집니다.  

### Business Rules and Arm Availability  

policy가 action을 선택해도 여러 비즈니스 규칙과 상황으로 인해 그 action이 실제로 추천 리스트에 없을 수도 있습니다. 이렇게 되면 policy의 exploration 능력은 제한될 수 밖에 없습니다. 추가적으로 로그 데이터의 편향을 제거하는 것은 굉장히 어려운 일이 됩니다. 관련해서 이러한 문제를 다룬 연구들이 있습니다. 논문 본문을 참고하면 좋을 것 같네요.  

### EXTENSIONS TO OPEN BANDIT PIPELINE  

앞서 설명했던 여러 문제를 해결하기 위해서는 쉽게 실험을 해볼 수 있어야 합니다. 논문에서는 아래 class 개발을 통해 좀 더 쉽게 테스트를 진행할 수 있도록 기여했다고 기술하고 있습니다.  

- BanditEnvironmentSimulator and BanditPolicySimulator  
- CoefficientDrifter  
- ExponentialDelaySampler  




## References  
- [RecSys 2022 - Recap, Favorite Papers, and Lessons](https://eugeneyan.com/writing/recsys2022/)  

